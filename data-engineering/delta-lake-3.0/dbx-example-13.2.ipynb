{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e86b563-6d27-4697-98f4-9d06fe0a062a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# In this Notebook\n",
    "- **Create a new delta table with Uniform support for Iceberg**\n",
    "    - Validate Delta Metadata\n",
    "    - Validate Iceberg Metadata\n",
    "    - Read from Snowflake\n",
    "    - Read from Google \n",
    "\n",
    "- **Convert an existing delta table to Uniform**\n",
    "- **Benchmark**\n",
    "    - Table Writes \n",
    "    - Upserts\n",
    "    - Table & Metadata size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c03384e5-a811-44e5-be08-fa3c5cfc9077",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use when using Spark Connect \n",
    "\n",
    "# from databricks.connect import DatabricksSession\n",
    "# from databricks.sdk.core import Config\n",
    "\n",
    "# runtime_131 = '0601-182128-dcbte59m'\n",
    "# runtime_132 = '0630-162806-47pefcs5'\n",
    "\n",
    "# # Use DEFAULT configuration and pass an existing cluster_id\n",
    "# # Cluster Runtime > 13.2 \n",
    "# config = Config(\n",
    "#     profile = 'DEFAULT',\n",
    "#   cluster_id = runtime_132\n",
    "# )\n",
    " \n",
    "# spark = DatabricksSession.builder.sdkConfig(config).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "409b697c-72da-46bc-8580-02b47eed1040",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da7cd936-4b3f-41ba-adc6-8da7f0cad947",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_location(table_name):\n",
    "    s_loc = (\n",
    "        spark.sql(f\"describe extended {sample_table_name}\")\n",
    "        .filter(\"col_name='Location'\")\n",
    "        .collect()[0][\"data_type\"]\n",
    "    )\n",
    "\n",
    "    s_delta_loc = f\"{s_loc}/_delta_log\"\n",
    "    s_iceberg_log = f\"{s_loc}/metadata\"\n",
    "\n",
    "    # Display the Table Location, files within s_delta_loc and files within iceberg metadata\n",
    "    display(dbutils.fs.ls(s_loc)), display(dbutils.fs.ls(s_delta_loc)), display(\n",
    "        dbutils.fs.ls(s_iceberg_log)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17de1f68-e486-4b2a-bfc9-5ff5e24a2ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def replace_s3_bucket_name(file_info_list):\n",
    "    new_list = []\n",
    "    for file_info in file_info_list:\n",
    "        new_info = {\n",
    "            \"name\": file_info.name,\n",
    "            \"size\": file_info.size,\n",
    "            \"path\": re.sub(r\"s3://.*?/\", \"s3://<hidden-bucket>/\", file_info.path),\n",
    "        }\n",
    "        new_list.append(new_info)\n",
    "    return new_list\n",
    "\n",
    "\n",
    "def display_location_hidden(table_name):\n",
    "    s_loc = (\n",
    "        spark.sql(f\"describe extended {sample_table_name}\")\n",
    "        .filter(\"col_name='Location'\")\n",
    "        .collect()[0][\"data_type\"]\n",
    "    )\n",
    "\n",
    "    s_delta_loc = f\"{s_loc}/_delta_log\"\n",
    "    s_iceberg_log = f\"{s_loc}/metadata\"\n",
    "\n",
    "    # Display the Table Location, files within s_delta_loc and files within iceberg metadata\n",
    "    display(replace_s3_bucket_name(dbutils.fs.ls(s_loc))), display(\n",
    "        replace_s3_bucket_name(dbutils.fs.ls(s_delta_loc))\n",
    "    ), display(replace_s3_bucket_name(dbutils.fs.ls(s_iceberg_log)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8816f680-6e6e-436f-908f-d3b55034b025",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create a new Delta Table with Universal (Iceberg support) \n",
    "[Official Documentation](https://docs.delta.io/3.0.0rc1/delta-uniform.html)\n",
    "\n",
    "**Requirements: Databricks Runtime 13.2 and above**\n",
    "\n",
    "- The table must have column mapping enabled. See Delta column mapping.\n",
    "- The Delta table must have a minReaderVersion >= 2 and minWriterVersion >= 7.\n",
    "- Writes to the table must use Delta Lake 3.0 or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a63517c-6675-47e5-a715-4c22bc162f56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"rohit_play_area\")\n",
    "dbutils.widgets.text(\"schema\", \"sandbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76faeec7-f3a9-443f-b33e-b458a7d9ed7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog_name\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "spark.sql(f\"create catalog if not exists {catalog}\")\n",
    "spark.sql(f\"use catalog {catalog}\")\n",
    "\n",
    "spark.sql(f\"create schema if not exists {schema}\")\n",
    "spark.sql(f\"use schema {schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b90c47f-a302-4624-a2f5-7371f59c0347",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Enabling Delta UniForm sets the Delta table feature **IcebergCompatV1**, a write protocol feature. \n",
    "Only clients that support this table feature can write to enabled tables. You must use Delta Lake 3.0 or above to write to Delta tables with this feature enabled.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e34810-05fd-4fec-8876-f275afadd44e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Universal Format is only supported on Unity Catalog tables.\n",
    "sample_table_name = \"iceberg_test_table\"\n",
    "spark.sql(f\"drop table if exists {sample_table_name}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {sample_table_name}(name STRING, age INT) \n",
    "    USING DELTA \n",
    "    TBLPROPERTIES('delta.universalFormat.enabledFormats' = 'iceberg')\n",
    "\"\"\"\n",
    ")\n",
    "spark.sql(f\"insert into {sample_table_name} values ('Amitabh', 73)\")\n",
    "spark.sql(f\"insert into {sample_table_name} values ('Dilip', 85)\")\n",
    "spark.sql(f\"insert into {sample_table_name} values ('Dev', 53)\")\n",
    "\n",
    "spark.sql(f\"select * from {sample_table_name}\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0483cf01-ac89-4fe1-9a58-738bcf295644",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the table properties in detail\n",
    "spark.sql(f\"describe extended {sample_table_name}\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ef4947-a9a4-42b3-99ce-ee3f0d0b5472",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"SHOW TBLPROPERTIES {sample_table_name}\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
<<<<<<< Updated upstream
=======
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c660e65-b594-4617-9116-551eb4a46587",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display_location_hidden(sample_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
>>>>>>> Stashed changes
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3472e26-720e-4d4f-aa37-2b65280ef8c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the Files now\n",
    "\n",
    "# Get the file location\n",
    "s_loc = (\n",
    "    spark.sql(f\"describe extended {sample_table_name}\")\n",
    "    .filter(\"col_name='Location'\")\n",
    "    .collect()[0][\"data_type\"]\n",
    ")\n",
    "\n",
    "s_delta_loc = f\"{s_loc}/_delta_log\"\n",
    "s_iceberg_log = f\"{s_loc}/metadata\"\n",
    "\n",
    "# Display the Table Location, files within s_delta_loc and files within iceberg metadata\n",
    "display(dbutils.fs.ls(s_loc)), display(dbutils.fs.ls(s_delta_loc)), display(\n",
    "    dbutils.fs.ls(s_iceberg_log)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b11ebf45-8b04-496d-9df5-31aa7e78f491",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Convert an existing Delta Table to support Uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "592fb83b-bfcb-4b10-9dbd-02506063ff39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Copy an existing Delta Table and then convert to Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae8eb352-b445-47c0-a267-0d604091a41b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy an existing delta table in our sandbox\n",
    "\n",
    "# We'll use the samples database and first copy the Orders table to our schema as a Delta Table. As expected the delta table should have data files and _delta_log\n",
    "\n",
    "source_table = \"samples.tpch.orders\"\n",
    "target_table = \"orders_delta\"\n",
    "\n",
    "spark.sql(f\"drop table if exists {target_table}\")\n",
    "\n",
    "# Write using overwrite mode\n",
    "spark.table(source_table).write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    target_table\n",
    ")\n",
    "\n",
    "# Let's look at the Files now\n",
    "\n",
    "# Get the file location\n",
    "t_loc = (\n",
    "    spark.sql(f\"describe extended {target_table}\")\n",
    "    .filter(\"col_name='Location'\")\n",
    "    .collect()[0][\"data_type\"]\n",
    ")\n",
    "t_delta_loc = f\"{t_loc}/_delta_log\"\n",
    "t_iceberg_loc = f\"{t_loc}/metadata\"\n",
    "\n",
    "spark.sql(f\"describe extended {target_table}\").display(), display(\n",
    "    dbutils.fs.ls(t_loc)\n",
    "), display(dbutils.fs.ls(t_delta_loc)), t_iceberg_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f73012e3-f15c-4573-8cbc-8b540987c449",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"describe extended {target_table}\").display(), display(\n",
    "    dbutils.fs.ls(t_loc)\n",
    "), display(dbutils.fs.ls(t_delta_loc)), display(dbutils.fs.ls(t_iceberg_loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87104cfa-7228-4f04-a112-d1577e9eba97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Convert using Alter Table\n",
    "\n",
    "Let's try to convert the delta table using the Alter table syntax\n",
    "\n",
    "**Notes** \n",
    "- Table Protocol must support changing column mapping modes using delta.columnMappingMode\n",
    "- Requires IcebergCompatV1 to be manually enabled in order for Universal Format (Iceberg) to be enabled on an existing table\n",
    "\n",
    "**Required Delta protocol version for column mapping:Protocol(2,5)**\n",
    "\n",
    "\n",
    "UniForm automatically generates Iceberg metadata **asynchronously**, allowing Iceberg clients to read Delta tables as if they were Iceberg tables. \n",
    "You can expect negligible Delta write overhead when UniForm is enabled, as the **Iceberg conversion and transaction occurs asynchronously after the Delta commit**.\n",
    "\n",
    "\n",
    "###When does UniForm generate Iceberg metadata?\n",
    "\n",
    "Delta Lake triggers Iceberg metadata generation asynchronously after a Delta Lake write transaction completes using the same compute that completed the Delta transaction.\n",
    "Iceberg can have significantly higher write latencies than Delta Lake. Delta tables with frequent commits might bundle multiple Delta commits into a single Iceberg commit.\n",
    "Delta Lake ensures that only one Iceberg metadata generation process is in progress at any time in a single cluster. \n",
    "Commits that would trigger a second concurrent Iceberg metadata generation process successfully commit to Delta, but do not trigger asynchronous Iceberg metadata generation. This prevents cascading latency for metadata generation for workloads with frequent commits (seconds to minutes between commits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdd69827-0098-4af2-abf4-950bf05bd5c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "ALTER TABLE {target_table} SET TBLPROPERTIES(\n",
    "   'delta.columnMapping.mode' = 'name',\n",
    "   'delta.minReaderVersion' = '2',\n",
    "   'delta.minWriterVersion' = '5')\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "ALTER TABLE {target_table} SET TBLPROPERTIES(\n",
    "  'delta.enableIcebergCompatV1' = 'true')\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "ALTER TABLE {target_table} SET TBLPROPERTIES(\n",
    "  'delta.columnMapping.mode' = 'name',\n",
    "  'delta.universalFormat.enabledFormats' = 'iceberg')\n",
    "  \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe0b785-60fa-48d2-a2af-a12523302fa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the Files again, we should have a folder with Iceberg metadata\n",
    "target_table = \"orders_delta\"\n",
    "display_location_hidden(target_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1248294c-9f92-462a-b142-28a46777efdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"SHOW TBLPROPERTIES {target_table}\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60ccd95a-6cdb-4763-8d11-90d034541570",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Write a Delta table with Universal Format enabled \n",
    "\n",
    "When writing a table using spark we can also write directly enabling the Universal format\n",
    "\n",
    "**Use _delta.universalFormat.enabledFormats = iceberg option_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb64ec5-7852-4fa5-b2e7-2a12cf8e2c38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_table_uniform = \"orders_uniform\"\n",
    "\n",
    "spark.table(source_table).write.format(\"delta\").mode(\"overwrite\").option(\n",
    "    \"delta.universalFormat.enabledFormats\", \"iceberg\"\n",
    ").saveAsTable(target_table_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9148753-88d0-44d1-b9a1-3ca1a038feeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "t_uni_loc = (\n",
    "    spark.sql(f\"describe extended {target_table_uniform}\")\n",
    "    .filter(\"col_name='Location'\")\n",
    "    .collect()[0][\"data_type\"]\n",
    ")\n",
    "t_uni_delta_loc = f\"{t_uni_loc}/_delta_log\"\n",
    "t_uni_iceberg_loc = f\"{t_uni_loc}/metadata\"\n",
    "\n",
    "\n",
    "# Let's look at the Files again, we should have a folder with Iceberg metadata\n",
    "spark.sql(f\"describe extended {target_table_uniform}\").display(), display(\n",
    "    dbutils.fs.ls(t_uni_loc)\n",
    "), display(dbutils.fs.ls(t_uni_delta_loc)), display(dbutils.fs.ls(t_uni_iceberg_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f62c0f3-358d-4892-9a18-05ed41d1c5e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's try to write some data to this table\n",
    "spark.sql(f\"\"\"\n",
    "          insert into {target_table_uniform}\n",
    "          select * from {target_table_uniform} limit 100 \n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf0f18e-475e-4370-b06a-14cfde161c68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the Files again, we should have a folder with Iceberg metadata\n",
    "spark.sql(f\"describe extended {target_table_uniform}\").display(), display(\n",
    "    dbutils.fs.ls(t_uni_loc)\n",
    "), display(dbutils.fs.ls(t_uni_delta_loc)), t_uni_iceberg_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "393ebf4b-aa42-458a-9c87-fea74e18221c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Test Write speed\n",
    "\n",
    "Test writing 10GB data to a Delta table with and without Universal format enabled \n",
    "\n",
    "**Note the sample source data may not be available in all workspaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
<<<<<<< Updated upstream
=======
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab88c0e4-67a2-41d4-a8dd-ed29b5261907",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def collect_write_stats(df, tgt_table, uni_flag):\n",
    "    start_time = time.time()\n",
    "\n",
    "    if uni_flag:\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").option(\n",
    "            \"delta.universalFormat.enabledFormats\", \"iceberg\"\n",
    "        ).saveAsTable(tgt_table)\n",
    "    else:\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(tgt_table)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    output = f\"\\n Total time = {end_time - start_time} seconds\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
>>>>>>> Stashed changes
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e9dcfa-ddf9-4823-976f-f0750acde0ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_10gb = \"tpcds.delta_100gb_nopartitions_nostats.store_sales\"\n",
    "t_10gb = \"store_sales\"\n",
    "t_10gb_uni = \"store_sales_uni\"\n",
    "\n",
    "\n",
    "spark.sql(f\"drop table if exists {t_10gb}\")\n",
    "spark.sql(f\"drop table if exists {t_10gb_uni}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
<<<<<<< Updated upstream
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab88c0e4-67a2-41d4-a8dd-ed29b5261907",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def collect_write_stats(df, tgt_table, uni_flag):\n",
    "  start_time = time.time()\n",
    "\n",
    "  if uni_flag:\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").option(\"delta.universalFormat.enabledFormats\", \"iceberg\").saveAsTable(tgt_table)\n",
    "  else:\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(tgt_table)\n",
    "\n",
    "  end_time = time.time()\n",
    "\n",
    "  output = f\"\\n Total time = {end_time - start_time} seconds\"\n",
    "  return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
=======
>>>>>>> Stashed changes
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d620112f-bf73-41d7-8b1b-bd7f726eff09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's cache the dataframe first\n",
    "source_10gb_df = spark.table(source_10gb).cache()\n",
    "source_10gb_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a09a763-8020-4ca9-a329-f3f3fa5e3bac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "collect_write_stats(source_10gb_df, t_10gb, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb08778-2c36-4a9a-b453-52be6c1460e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "collect_write_stats(source_10gb_df, t_10gb_uni, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6252c160-a6da-4add-92f3-d10cfacbea82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "t_10gb_loc = (\n",
    "    spark.sql(f\"describe extended {t_10gb}\")\n",
    "    .filter(\"col_name='Location'\")\n",
    "    .collect()[0][\"data_type\"]\n",
    ")\n",
    "\n",
    "display(dbutils.fs.ls(t_10gb_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2c6cc02-ccb7-4fd3-a068-ebc2adf799d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "t_10gb_uni_loc = (\n",
    "    spark.sql(f\"describe extended {t_10gb_uni}\")\n",
    "    .filter(\"col_name='Location'\")\n",
    "    .collect()[0][\"data_type\"]\n",
    ")\n",
    "\n",
    "display(dbutils.fs.ls(t_10gb_uni_loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d67ee60-f20a-409b-a906-e356ed9df75d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Limitations\n",
    "\n",
    "**UniForm is read-only from an Iceberg perspective, though this cannot yet be enforced since UniForm uses file-system-based commits when writing Iceberg. If any external writer (not Delta Lake) writes to this Iceberg table, this may destroy your Delta table and cause data loss, as the Iceberg writer may perform data cleanup or garbage collection that Delta is unaware of.**\n",
    "\n",
    "The following limitations exist:\n",
    "- UniForm does not work on tables with deletion vectors enabled. See What are deletion vectors?.\n",
    "- Delta tables with UniForm enabled do not support LIST, MAP, and VOID types.\n",
    "- Iceberg clients can only read from UniForm. Writes are not supported.\n",
    "- Iceberg reader clients might have individual limitations, regardless of UniForm. See documentation for your target client.\n",
    "- Iceberg reader clients version 1.2.0 and below do not support INT96 timestamp type written by Apache Spark. Use the following code in notebooks that write to UniForm tables to avoid this limitation: spark.conf.set(“spark.sql.parquet.outputTimestampType”, “TIMESTAMP_MICROS”)\n",
    "\n",
    "The following Delta Lake features work for Delta clients when UniForm is enabled, but do not have support in Iceberg:\n",
    "- Change Data Feed\n",
    "- Delta Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ced033a-b943-464a-b1a3-2e621a8cae64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5caa3d76-f80c-4c84-af3c-f9f8db734ea6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(f\"drop table if exists {sample_table_name}\")\n",
    "# spark.sql(f\"drop table if exists {target_table}\")\n",
    "# spark.sql(f\"drop table if exists {target_table_uniform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3efbb4b5-cd4a-4639-9b01-6f64ec09d403",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 969987237406814,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "dbx-example-13.2",
   "widgets": {
    "catalog_name": {
     "currentValue": "rohit_play_area",
     "nuid": "3f13c0a5-adba-49c8-abf8-67e37dec8315",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "rohit_play_area",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "sandbox",
     "nuid": "2cf7207d-980e-4a3d-9fb2-c63d980fdd7f",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sandbox",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
